import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import config

def load_model_and_tokenizer(model_path=config.MODEL_PATH):
    # KIá»‚M TRA NGUá»’N MODEL
    if os.path.exists(model_path):
        print(f">> [Loader] âœ… TÃ¬m tháº¥y Model trÃªn Google Drive: {model_path}")
        print(">> Äang táº£i tá»« á»• cá»©ng (Sáº½ ráº¥t nhanh)...")
        load_source = model_path
    else:
        print(f">> [Loader] âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c: {model_path}")
        print(f">> [Loader] ğŸ”„ Chuyá»ƒn sang táº£i tá»« Hugging Face (Sáº½ tá»‘n thá»i gian)...")
        load_source = config.HF_MODEL_ID

    # Báº®T Äáº¦U Táº¢I
    try:
        tokenizer = AutoTokenizer.from_pretrained(load_source)
        tokenizer.pad_token = tokenizer.eos_token
        
        # LOGIC Tá»° Äá»˜NG NHáº¬N DIá»†N MÃ”I TRÆ¯á»œNG
        if torch.cuda.is_available():
            print(">> [System] PhÃ¡t hiá»‡n GPU NVIDIA (Colab). KÃ­ch hoáº¡t 4-bit...")
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                load_source,
                quantization_config=bnb_config,
                device_map="auto",
                local_files_only=True if os.path.exists(model_path) else False 
                # ^ DÃ²ng trÃªn Ã©p buá»™c dÃ¹ng file local náº¿u Ä‘Æ°á»ng dáº«n tá»“n táº¡i
            )
            device = "cuda"

        elif torch.backends.mps.is_available():
            print(">> [System] PhÃ¡t hiá»‡n Mac M-Chip. KÃ­ch hoáº¡t MPS...")
            model = AutoModelForCausalLM.from_pretrained(
                load_source,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            device = "mps"
            
        else:
            print(">> [System] Cháº¡y báº±ng CPU...")
            model = AutoModelForCausalLM.from_pretrained(load_source)
            device = "cpu"

        print(">> [Loader] Táº£i model thÃ nh cÃ´ng!")
        return model, tokenizer, device

    except Exception as e:
        print(f"\n>> [FATAL ERROR] Lá»—i khi táº£i model: {e}")
        print(">> HÃ£y kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n Google Drive hoáº·c quyá»n truy cáº­p.")
        raise e