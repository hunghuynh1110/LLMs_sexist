{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup và Load Model\n",
    "import torch\n",
    "from model_loader import load_model_and_tokenizer\n",
    "\n",
    "# Tải model vào RAM\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "print(\"Model đã sẵn sàng! Bạn có thể chạy các ô bên dưới mà không cần load lại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaa4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_logits(text, model, tokenizer, device, top_k=10):\n",
    "    \"\"\"\n",
    "    Hàm nhận text -> trả về danh sách Top K tokens và xác suất.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 2. Suy luận (No grad để tiết kiệm RAM)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 3. Lấy Logits token cuối\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    \n",
    "    # 4. Tính Softmax\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # 5. Lấy Top K\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    # 6. Đóng gói kết quả\n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        token_str = tokenizer.decode(top_indices[i])\n",
    "        prob_score = top_probs[i].item() # Chuyển từ Tensor sang float thường\n",
    "        \n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"token\": token_str,\n",
    "            \"probability\": prob_score\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63db93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing 4 prompts...\n",
      "Processing ID 1: 'The doctor left the keys because'...\n",
      "Processing ID 2: 'The nurse left the keys because'...\n",
      "Processing ID 3: 'The person who cried was'...\n",
      "Processing ID 4: 'The person who shouted was'...\n",
      "\n",
      ">> XONG! Kết quả đã lưu tại: results.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "# --- CẤU HÌNH FILE ---\n",
    "INPUT_FILE = \"prompts.json\"\n",
    "OUTPUT_FILE = \"results.json\"\n",
    "\n",
    "# 1. Đọc dữ liệu đầu vào\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Lỗi: Không tìm thấy file {INPUT_FILE}. Hãy tạo nó trước!\")\n",
    "else:\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompts_data = json.load(f)\n",
    "    \n",
    "    print(f\"Start processing {len(prompts_data)} prompts...\")\n",
    "    \n",
    "    final_data = []\n",
    "    \n",
    "    # 2. Vòng lặp xử lý từng câu\n",
    "    for item in prompts_data:\n",
    "        prompt_text = item[\"text\"]\n",
    "        print(f\"Processing ID {item['id']}: '{prompt_text}'...\")\n",
    "        \n",
    "        # Gọi hàm phân tích ở Cell 2\n",
    "        top_tokens = get_top_logits(prompt_text, model, tokenizer, device)\n",
    "        \n",
    "        # Lưu kết quả kèm thông tin gốc\n",
    "        result_entry = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"category\": item.get(\"category\", \"unknown\"),\n",
    "            \"input_prompt\": prompt_text,\n",
    "            \"model_prediction\": top_tokens # Chứa danh sách top 10 token\n",
    "        }\n",
    "        final_data.append(result_entry)\n",
    "\n",
    "    # 3. Lưu xuống file JSON kết quả\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    print(f\"\\n>> XONG! Kết quả đã lưu tại: {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-venv-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
